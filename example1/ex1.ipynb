{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get/Prepare the data\n",
    "Let us create a training and testing datasets.  \n",
    "The data is created randomly so the testing and training datasets do not intersect  \n",
    "The data will be normalized to the [0,1] interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import data  # hand-made library with a couple of functions\n",
    "# Create the data\n",
    "IN_train, OUT_train = data.gen_data_cool(300, norm=True)\n",
    "IN_test, OUT_test = data.gen_data_cool(100, norm=True)\n",
    "\n",
    "# Visualize the data\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(IN_train, OUT_train, label='training')\n",
    "ax.scatter(IN_test, OUT_test, label='testing')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "Let us create a model with a single hidden layer formed by 5 sigmoid neurons. It would look something like this:\n",
    "<img src=\"test.svg\" />\n",
    "The output of the NN can be calculated as follows:\n",
    "$$\\text{Output} = \\vec{a}_3 = \\sigma\\left( W_3\\cdot\\vec{a}_2 +b_3 \\right)$$\n",
    "where $\\vec{a}_2$ is the output (activation) of the second hidden layer:\n",
    "$$\\vec{a}_2 = \\sigma\\left( W_2\\cdot\\vec{a}_1 +\\vec{b}_2 \\right)$$\n",
    "where $\\vec{a}_1$ is the output of the first hidden layer:\n",
    "$$\\vec{a}_1 = \\sigma\\left( W_1\\cdot\\vec{v}_{\\text{input}} +\\vec{b}_1 \\right)$$\n",
    "\n",
    "The NN, then, will be defined by two weight matrices ($W_1$ and $W_2$ in the figure) and two biases arrays (one for the hidden layer and one for the output layer) with shapes:\n",
    "$$W_1\\rightarrow(1,2) \\qquad W_2\\rightarrow(2,5) \\qquad W_3\\rightarrow(5,1)$$\n",
    "$$\\vec{b}_1\\rightarrow(1,2) \\qquad \\vec{b}_2\\rightarrow(1,5) \\qquad \\vec{b}_3\\rightarrow(1,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([keras.layers.Dense(2, activation=None, input_shape=(1,)),\n",
    "                          keras.layers.Dense(5, activation=tf.nn.sigmoid),\n",
    "                          keras.layers.Dense(1, activation=None) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'mean_squared_error',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the model, initialized randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Print the model weights and biases\n",
    "#\n",
    "for W in model.get_weights():\n",
    "    print(W.shape)\n",
    "    print(W)\n",
    "    print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "from time import time\n",
    "told = time()\n",
    "history = model.fit(IN_train, OUT_train, epochs=1000,\n",
    "                    validation_data = (IN_test,OUT_test),\n",
    "                    verbose=0)\n",
    "print('Training done in: %.3f s'%(time()-told))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "err = history.history['loss']\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(err,label='loss',lw=2)\n",
    "ax.set_title('Learning curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction over the whole domain\n",
    "x_predict = np.linspace(np.min(IN_train),np.max(IN_train),500)\n",
    "x_predict = data.lineal_norm(x_predict)\n",
    "\n",
    "y_predict = model.predict(x_predict)\n",
    "y_predict = data.lineal_norm(y_predict)   # should be unnecessary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(IN_train, OUT_train, label='train')\n",
    "ax.scatter(IN_test,  OUT_test,label='test')\n",
    "ax.plot(x_predict, y_predict,'k',lw=2, label='prediction')\n",
    "ax.legend()\n",
    "ax.set_title('Results')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "model.save('my_first_NN.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
