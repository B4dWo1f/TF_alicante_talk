{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, only for Jupyter\n",
    "%matplotlib notebook\n",
    "\n",
    "# General libraries\n",
    "import numpy as np                # to deal with arrays, vectors, matrices...\n",
    "import matplotlib.pyplot as plt   # to plot the data\n",
    "# Tensorflow\n",
    "import os\n",
    "HOME = os.getenv('HOME')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # to get rid of the TF compilation warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only because my system-wide config is tuned, you don't need these lines\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = 5,3\n",
    "mpl.rcParams['font.size'] = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the data\n",
    "In this example we load some data manually created. It consists of 2 inputs and 1 output.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and testing data\n",
    "n_out = 1   # number of outputs\n",
    "\n",
    "M = np.loadtxt('Spola.train')\n",
    "IN = M[:,:-n_out]\n",
    "OUT = M[:,-n_out:].astype(int)\n",
    "\n",
    "M = np.loadtxt('Spola.test')\n",
    "IN_test = M[:,:-n_out]\n",
    "OUT_test = M[:,-n_out:].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----> Read data samples <----')\n",
    "print(f'    Training --> Input: {IN.shape}, Output: {OUT.shape}')\n",
    "print(f'  Validation --> Input: {IN_test.shape}, Output: {OUT_test.shape}')\n",
    "inp_shape = IN.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.subplot(111, projection='polar')\n",
    "\n",
    "# Plot Expected outputs\n",
    "# this is just a fancy polar plot, the relevant lines are just the scatter\n",
    "X = IN_test[:,0]\n",
    "Y = IN_test[:,1]\n",
    "Z = OUT_test.flatten()\n",
    "ax.scatter(Y,X,c=Z)  # plot the expected result\n",
    "ax.set_rmax(36)\n",
    "ax.set_theta_zero_location(\"N\")\n",
    "ax.set_title('Expected output',y=1.1)\n",
    "ax.set_xticklabels(['N', '', 'W', '', 'S', '', 'E', ''])\n",
    "ax.set_rlabel_position(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "About the `softmax` activation  \n",
    "The `softmax` function (check it in the documentation or in wikipedia) is exensively used in classification problems, the dimension of the layer has to be the number of classes in the problem.\n",
    "In our case we have 3 classes (the output is either 0, 1 or 2), so the output will be either:  \n",
    "- [1,0,0] --> 0\n",
    "- [0,1,0] --> 1\n",
    "- [0,0,1] --> 2  \n",
    "\n",
    "The `softmax` layer takes a random vector a transform it into probabilities. The sum of the outputs is `1` each element can be interpreted as the probability for that output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add( Dense(3, activation='tanh', input_shape=inp_shape) )\n",
    "model.add( Dense(5, activation='tanh') )\n",
    "model.add( Dense(10, activation='tanh') )\n",
    "model.add( Dense(3, activation='softmax') )  # this dimension should be equal to the number of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems crossentropy is a much better loss function.\n",
    "- `binary_crossentropy` is the most suited function for classifiying the inputs into 2  classes\n",
    "- `categorical_crossentropy` is the most suited function for classifiying the inputs into a number (>2) of classes\n",
    "- `sparse_categorical_crossentropy` is the same function as the `categorical_crossentropy`, it only differs in the format of the data.\n",
    "  - `categorical_crossentropy` requires the output as a vector (for instance: [1,0,0])\n",
    "  - `sparse_categorical_crossentropy` requires the output as an integer that denotes the element of the vector which is one:  0 --> [1,0,0] ; 1 --> [0,1,0] ; 2 --> [0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'sparse_categorical_crossentropy',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(IN, OUT, epochs = 500,\n",
    "                    validation_data = (IN_test,OUT_test),\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(err,label='loss')\n",
    "ax.plot(acc,label='accuracy')\n",
    "ax.set_title('Learning curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate & plot results\n",
    "OUT_predict = model.predict(IN_test)\n",
    "OUT_predict = np.argmax(OUT_predict,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax0 = plt.subplot(121, projection='polar')   # expected\n",
    "ax1 = plt.subplot(122, projection='polar')   # predicted\n",
    "\n",
    "#fig.suptitle(\"$Neural$ $Network$\", fontsize=30)\n",
    "\n",
    "# Plot Expected outputs\n",
    "X = IN_test[:,0]\n",
    "Y = IN_test[:,1]\n",
    "Z = OUT_test.flatten()\n",
    "ax0.scatter(Y,X,c=Z)\n",
    "\n",
    "# Plot Predicted outputs\n",
    "X = IN_test[:,0]\n",
    "Y = IN_test[:,1]\n",
    "Z = OUT_predict.flatten()\n",
    "ax1.scatter(Y,X,c=Z)  # plot the predicted result\n",
    "\n",
    "# Making the plot look fancier\n",
    "ax0.set_rmax(36)\n",
    "ax0.set_theta_zero_location(\"N\")\n",
    "ax0.set_title('Expected output',y=1.1)\n",
    "ax0.set_xticklabels(['N', '', 'W', '', 'S', '', 'E', ''])\n",
    "ax0.set_rlabel_position(45)\n",
    "\n",
    "# Settings\n",
    "ax1.set_rmax(36)\n",
    "ax1.set_theta_zero_location(\"N\")\n",
    "ax1.set_title('Predicted output',y=1.1)\n",
    "ax1.set_xticklabels(['N', '', 'W', '', 'S', '', 'E', ''])\n",
    "ax1.set_rlabel_position(45)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('spola.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
