{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest example\n",
    "## XOR gate\n",
    "The XOR gate is a function of two variables that returns one output:\n",
    "$$\\begin{array}{cc|c}\n",
    "x_1 & x_2 & y \\\\ \\hline\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "In this case we create it manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the XOR behavior\n",
    "IN_train = np.array([[0,0],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,1]])\n",
    "\n",
    "OUT_train = np.array([[0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test data, since the inuputs are constrained to $\\{0,1\\}$. We use the training set as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_test = IN_train\n",
    "OUT_test = OUT_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the NN\n",
    "keras.Sequential is the class used to create MLP  \n",
    "keras.layers.Dense is the standard layer, and sigmoid the standard activation.  \n",
    "The sizes of these layers are:\n",
    "    $$input\\times layer_1 \\quad;\\quad layer_1\\times output$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([\n",
    "        keras.layers.Dense(3, activation=tf.nn.sigmoid, input_shape=(2,)),\n",
    "        keras.layers.Dense(1, activation=None) ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model and check that everything is ok.  \n",
    "optimizer: method to follow the gradient descent.  \n",
    "loss: error function to use.  \n",
    "metrics: statistics to keep in order to monitor the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'mean_squared_error',\n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test now our model, initialized with random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before training:')\n",
    "print('Input   xpct Out   Output')\n",
    "#\n",
    "# We use the \"predict\" method to evaluate the model in the training dataset\n",
    "#\n",
    "predicted = model.predict(IN_train)\n",
    "for i in range(IN_train.shape[0]):\n",
    "   print(IN_train[i],'   ',OUT_train[i],'      %.2f'%(predicted[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "The training process is carried out by the \"fit\" method.  \n",
    "epochs: Number of steps towards the error minimum  \n",
    "validation_data: If available, it is the dataset against which the accuracy is measured\n",
    "verbose: 0 run quietly, no output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(IN_train, OUT_train, epochs=5000,\n",
    "                    validation_data = (IN_test,OUT_test),\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "err = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(err,label='loss')\n",
    "ax.plot(acc,label='accuracy')\n",
    "ax.set_title('Learning curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('')\n",
    "print('After training:')\n",
    "print('Input   xpct Out   Output')\n",
    "predicted = model.predict(IN_train)\n",
    "for i in range(IN_train.shape[0]):\n",
    "   print(IN_train[i],'   ',OUT_train[i],'      %.2f'%(predicted[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
