{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simplest example\n",
    "## XOR gate\n",
    "- Traditional problem in the early days of AI research\n",
    "- The XOR gate is a function of two variables that returns one output:\n",
    "$$\\begin{array}{cc|c}\n",
    "x_1 & x_2 & y \\\\ \\hline\n",
    "0 & 0 & 0 \\\\\n",
    "0 & 1 & 1 \\\\\n",
    "1 & 0 & 1 \\\\\n",
    "1 & 1 & 0\n",
    "\\end{array}$$\n",
    "\n",
    "Violates all the recomendations train/validation... but enough to learn how to use the libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional, only for Jupyter\n",
    "%matplotlib notebook\n",
    "\n",
    "# General libraries\n",
    "import numpy as np                # to deal with arrays, vectors, matrices...\n",
    "import matplotlib.pyplot as plt   # to plot the data\n",
    "# Tensorflow\n",
    "import os\n",
    "HOME = os.getenv('HOME')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # to get rid of the TF compilation warnings\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only because my system-wide config is tuned, you don't need these lines\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = 5,3\n",
    "mpl.rcParams['font.size'] = 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data\n",
    "In this case we create the dataset manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replicate the XOR behavior\n",
    "IN_train = np.array([[0,0],\n",
    "                     [0,1],\n",
    "                     [1,0],\n",
    "                     [1,1]])\n",
    "\n",
    "OUT_train = np.array([[0],\n",
    "                      [1],\n",
    "                      [1],\n",
    "                      [0]])\n",
    "\n",
    "inp_shape = IN_train.shape[1:]\n",
    "print(inp_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no test data. We use the training set as testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_test = IN_train\n",
    "OUT_test = OUT_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for normalization since all inputs and outputs $\\in[0,1]$  \n",
    "No NaN nor missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the NN\n",
    "`tensorflow.keras.models.Sequential` is the class used to create _Multi Layer Perceptrons_  \n",
    "`tensorflow.keras.layers.Dense` is the standard Fully Connected layer. The default activation is `None` so we change it to the standard `sigmoid`.  \n",
    "\n",
    "You can play around here, try different architectures, activations (sigmoid, tanh...), etc\n",
    "Possible architectures:\n",
    "- `input --> 2 --> 1 --> output`  (harder to train)  \n",
    "- `input --> 200 --> 500 --> 1000 --> 100 --> 1 --> output` (slower to train)\n",
    "- `input --> 10 --> 5 --> 3 --> 1 --> output` (reasonable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential()\n",
    "model.add(Dense(5, activation='sigmoid', input_shape=inp_shape))\n",
    "model.add(Dense(3, activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "# model.add(Dense(10, activation='tanh', input_shape=inp_shape))\n",
    "# model.add(Dense(5, activation='tanh'))\n",
    "# model.add(Dense(3, activation='tanh'))\n",
    "# model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compile the model and check that everything is ok.  \n",
    "`optimizer`: method to follow the gradient descent.  \n",
    "`loss`: error function to use.  \n",
    "`metrics`: statistics to keep in order to monitor the training process, can be other loss functions, or any other that can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam',\n",
    "              loss = 'mean_squared_error',\n",
    "              metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "We can test now our model, initialized with random parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before training:')\n",
    "print('Input   xpct Out   Output')\n",
    "#\n",
    "# We use the \"predict\" method to evaluate the model in the training dataset\n",
    "#\n",
    "predicted = model.predict(IN_train)\n",
    "\n",
    "for i in range(IN_train.shape[0]):\n",
    "    print(f'{IN_train[i]}     {OUT_train[i]}       {predicted[i][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained random results since the bias and weights were randomly chosen and the model had not been trained yet.\n",
    "Let us train the model now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model\n",
    "The training process is carried out by the \"fit\" method.  \n",
    "`epochs`: Number of steps towards the error minimum  \n",
    "`validation_data`: If available, it is the dataset against which the accuracy is measured  \n",
    "`verbose`: 0 run quietly, no output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "from time import time\n",
    "t_old = time()\n",
    "\n",
    "history = model.fit(IN_train, OUT_train, epochs=500,\n",
    "                    validation_data = (IN_test,OUT_test),\n",
    "                    verbose=1)\n",
    "\n",
    "print('Training: %ss'%(time()-t_old))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curve\n",
    "err = history.history['loss']\n",
    "acc = history.history['accuracy']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(err,label='loss')\n",
    "ax.plot(acc,label='accuracy')\n",
    "ax.set_title('Learning curves')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test now our model, with the optimized parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('After training:')\n",
    "print('Input   xpct Out   Output')\n",
    "predicted = model.predict(IN_train)\n",
    "for i in range(IN_train.shape[0]):\n",
    "    print(f'{IN_train[i]}     {OUT_train[i]}       {predicted[i][0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore parameter of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model weights and biases\n",
    "for W in model.get_weights():\n",
    "    print(W.shape)\n",
    "    print(W)\n",
    "    print('-----------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loaded = models.load_model('my_model.h5')\n",
    "model_loaded.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model_loaded.predict(IN_train)\n",
    "for i in range(IN_train.shape[0]):\n",
    "    print(IN_train[i],'   ',OUT_train[i],'      %.2f'%(predicted[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
